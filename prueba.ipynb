{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "from langchain.document_loaders import TextLoader  #for textfiles\n",
    "from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace models\n",
    "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
    "from langchain.vectorstores import FAISS  #facebook vectorizationfrom langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.document_loaders import UnstructuredPDFLoader  #load pdf\n",
    "from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import UnstructuredURLLoader  #load urls into docoument-loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "url2 = \"https://github.com/fabiomatricardi/cdQnA/raw/main/KS-all-info_rev1.txt\"\n",
    "res = requests.get(url2)\n",
    "with open(\"KS-all-info_rev1.txt\", \"w\") as f:\n",
    "  f.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('./KS-all-info_rev1.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"WHAT IS HIERARCHY 4.0\\nwhether you own build manage maintain or operate an oil plant inevitably issues arise that require immediate action and resolution.\\nWith big data flowing in constantly from all sectors making sense of everything while troubleshooting\\nissues without wasting time can be a huge challenge. \\nSo what's the solution?\\nintroducing hierarchy 4.0 and Innovative software solution for control Safety Systems \\nHierarchy 4.0 presents an interactive diagram of the entire plant revealing cause and effect Behavior with readings provided in a hierarchical view allowing for a deep understanding of the system's strategy \\nAll data is collected from multiple sources visualized as a diagram and optimized through a customized dashboard allowing users to run a logic simulation from live data or pick a moment from their history. \\nYour simulation is based on actual safety Logics not just on a math model \\nNow every users can prepare an RCA report 90 percent faster in just a few minutes.\\nHierarchy can be used for any project phase starting from engineering to commissioning and up to operation and maintenance while supporting hazop Hazard analysis by reducing human error and avoiding incorrect documentation.\\nHierarchy 4.0 supports plant operators in decision making taking into account both the safety and the operability of their assets. \\nHierarchy 4.0 Embraces a block log approach: it automatically calculates all Logics affected by an\\noverride and gives a full understanding of constraints. \\nNot convinced let's look at the data! \\nDuring its first project hierarchy 4.0 prevented a revenue loss of 45 million dollars. \\nPlants that utilize hierarchy 4.0 save up to 95 of their time and deliver a return on investment up to five times in value from day one and experience a Personnel utilization and plant efficiency increase by 20 percent per year.\\n\\nTry our demo and make the move to hierarchy 4.0 today\\n\\nCASE STUDIES\\nWe prepared four case studies to show the true potential of Hierarchy 4.0. In each case we are going to review the actual method to approach the issue and the new method, using Hierarchy 4.0, to solve the problem at hand.\\nThe four case studies are 1)Restore a PSD 1 node without requiring a planned shutdown; 2)Evaluate the maintenance on a voting 2 out of 3 instruments; 3)Prepare the maintenance on 2 out of 3 voting where one instrument is already in fault; 4) Prepare a full Root Cause Analysis (RCA) of an unexpected shutdown.\\n\\nCASE STUDY 1 - Restore a PSD 1 node without requiring a planned shutdown, all while the plant remains in normal operation.\\nThis Case Study Challenge is to restore a rack connected to the PSD 1 node without requiring a planned shutdown, all while the plant remains in normal operation. \\nWhat is the actual method: engineer and operator must use the instrument list the get the Rack 2 number of modules (9 AI modules) and verify how many channels they have assigned (8  channels for 9 cards for a total of 72 I/O). This is required to get all the Input/Output connected the Rack 2. All signals that have been identified must be verified in the logical configuration using an engineering workstation: 1)Each signal must be individually inspected in the system applications for its logic function. 2)If a signal is used in multiple controllers, all logics must be cross examined again for all the references. 3)The engineer must prepare a report containing the listed logics and final elements affected during this process.\\nWhat are the actual issues and drawbacks in using the actual method: 1)There is a possibility of human error when checking the connection between signals and logics. \\n2) The Impact Analysis Report has not been correctly updated with the latest software modifications. 3)The troubleshooting process is time consuming due to the involvement of several specialists. 4)Other maintenance activities have been delayed as a result.\\nThe new method is to use Hierarchy 4.0 and you will have your solution in few simple steps.1)Gain access to Hierarchy 4.0 2)Enter Maintenance mode and select the nodes PSD1 and RACK2  3)Obtain the results of your research in a few seconds 4)Get a comprehensive overview of all signals allocated in the specified controller, with emphasis on the signals involved in the rack replacement 5)Easily identify all  the associations between the items and the impacted logics 6)Print from Hierarchy all pages that are affected by the rack replacement 7)Maintain full control of the activity.\\n\\n\", metadata={'source': './KS-all-info_rev1.txt'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# tokens_integer = encoding.encode(docs[0].page_content)\n",
    "# print(f\"{len(tokens_integer)} is the number of tokens in my text\")\n",
    "\n",
    "# encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "# tokens_integer = encoding.encode(docs[0].page_content)\n",
    "# print(f\"{len(tokens_integer)} is the number of tokens in my text\")\n",
    "\n",
    "# encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "# tokens_integer = encoding.encode(docs[0].page_content)\n",
    "# print(f\"{len(tokens_integer)} is the number of tokens in my text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Text Splitter\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "# docs = text_splitter.split_documents(documents)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 374, which is longer than the specified 100\n",
      "Created a chunk of size 130, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"WHAT IS HIERARCHY 4.0\\nwhether you own build manage maintain or operate an oil plant inevitably issues arise that require immediate action and resolution.\\nWith big data flowing in constantly from all sectors making sense of everything while troubleshooting\\nissues without wasting time can be a huge challenge. \\nSo what's the solution?\\nintroducing hierarchy 4.0 and Innovative software solution for control Safety Systems \\nHierarchy 4.0 presents an interactive diagram of the entire plant revealing cause and effect Behavior with readings provided in a hierarchical view allowing for a deep understanding of the system's strategy \\nAll data is collected from multiple sources visualized as a diagram and optimized through a customized dashboard allowing users to run a logic simulation from live data or pick a moment from their history. \\nYour simulation is based on actual safety Logics not just on a math model \\nNow every users can prepare an RCA report 90 percent faster in just a few minutes.\\nHierarchy can be used for any project phase starting from engineering to commissioning and up to operation and maintenance while supporting hazop Hazard analysis by reducing human error and avoiding incorrect documentation.\\nHierarchy 4.0 supports plant operators in decision making taking into account both the safety and the operability of their assets. \\nHierarchy 4.0 Embraces a block log approach: it automatically calculates all Logics affected by an\\noverride and gives a full understanding of constraints. \\nNot convinced let's look at the data! \\nDuring its first project hierarchy 4.0 prevented a revenue loss of 45 million dollars. \\nPlants that utilize hierarchy 4.0 save up to 95 of their time and deliver a return on investment up to five times in value from day one and experience a Personnel utilization and plant efficiency increase by 20 percent per year.\", metadata={'source': './KS-all-info_rev1.txt'}),\n",
       " Document(page_content='Try our demo and make the move to hierarchy 4.0 today', metadata={'source': './KS-all-info_rev1.txt'}),\n",
       " Document(page_content='CASE STUDIES\\nWe prepared four case studies to show the true potential of Hierarchy 4.0. In each case we are going to review the actual method to approach the issue and the new method, using Hierarchy 4.0, to solve the problem at hand.\\nThe four case studies are 1)Restore a PSD 1 node without requiring a planned shutdown; 2)Evaluate the maintenance on a voting 2 out of 3 instruments; 3)Prepare the maintenance on 2 out of 3 voting where one instrument is already in fault; 4) Prepare a full Root Cause Analysis (RCA) of an unexpected shutdown.', metadata={'source': './KS-all-info_rev1.txt'}),\n",
       " Document(page_content='CASE STUDY 1 - Restore a PSD 1 node without requiring a planned shutdown, all while the plant remains in normal operation.\\nThis Case Study Challenge is to restore a rack connected to the PSD 1 node without requiring a planned shutdown, all while the plant remains in normal operation. \\nWhat is the actual method: engineer and operator must use the instrument list the get the Rack 2 number of modules (9 AI modules) and verify how many channels they have assigned (8  channels for 9 cards for a total of 72 I/O). This is required to get all the Input/Output connected the Rack 2. All signals that have been identified must be verified in the logical configuration using an engineering workstation: 1)Each signal must be individually inspected in the system applications for its logic function. 2)If a signal is used in multiple controllers, all logics must be cross examined again for all the references. 3)The engineer must prepare a report containing the listed logics and final elements affected during this process.\\nWhat are the actual issues and drawbacks in using the actual method: 1)There is a possibility of human error when checking the connection between signals and logics. \\n2) The Impact Analysis Report has not been correctly updated with the latest software modifications. 3)The troubleshooting process is time consuming due to the involvement of several specialists. 4)Other maintenance activities have been delayed as a result.\\nThe new method is to use Hierarchy 4.0 and you will have your solution in few simple steps.1)Gain access to Hierarchy 4.0 2)Enter Maintenance mode and select the nodes PSD1 and RACK2  3)Obtain the results of your research in a few seconds 4)Get a comprehensive overview of all signals allocated in the specified controller, with emphasis on the signals involved in the rack replacement 5)Easily identify all  the associations between the items and the impacted logics 6)Print from Hierarchy all pages that are affected by the rack replacement 7)Maintain full control of the activity.', metadata={'source': './KS-all-info_rev1.txt'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 is the number of tokens in my text\n",
      "374 is the number of tokens in my text\n",
      "374 is the number of tokens in my text\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens_integer = encoding.encode(docs[0].page_content)\n",
    "print(f\"{len(tokens_integer)} is the number of tokens in my text\")\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "tokens_integer = encoding.encode(docs[0].page_content)\n",
    "print(f\"{len(tokens_integer)} is the number of tokens in my text\")\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens_integer = encoding.encode(docs[0].page_content)\n",
    "print(f\"{len(tokens_integer)} is the number of tokens in my text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\damia\\Documents\\GitHub\\TalkDocument\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CASE STUDY 1 - Restore a PSD 1 node without requiring a planned shutdown, all while the plant remains in normal operation.\\nThis Case Study Challenge is to restore a rack connected to the PSD 1 node without requiring a planned shutdown, all while the plant remains in normal operation. \\nWhat is the actual method: engineer and operator must use the instrument list the get the Rack 2 number of modules (9 AI modules) and verify how many channels they have assigned (8  channels for 9 cards for a total of 72 I/O). This is required to get all the Input/Output connected the Rack 2. All signals that have been identified must be verified in the logical configuration using an engineering workstation: 1)Each signal must be individually inspected in the system applications for its logic function. 2)If a signal is used in multiple controllers, all logics must be cross examined again for all the references. 3)The engineer must prepare a report containing the listed logics and final elements affected during this process.\\nWhat are the actual issues and drawbacks in using the actual method: 1)There is a possibility of human error when checking the connection between signals and logics. \\n2) The Impact Analysis Report has not been correctly updated with the latest software modifications. 3)The troubleshooting process is time consuming due to the involvement of several specialists. 4)Other maintenance activities have been delayed as a result.\\nThe new method is to use Hierarchy 4.0 and you will have your solution in few simple steps.1)Gain access to Hierarchy 4.0 2)Enter Maintenance mode and select the nodes PSD1 and RACK2  3)Obtain the results of your research in a few seconds 4)Get a comprehensive overview of all signals allocated in the specified controller, with emphasis on the signals involved in the rack replacement 5)Easily identify all  the associations between the items and the impacted logics 6)Print from Hierarchy all pages that are affected by the rack replacement 7)Maintain full control of the activity.', metadata={'source': './KS-all-info_rev1.txt'}),\n",
       " Document(page_content='CASE STUDIES\\nWe prepared four case studies to show the true potential of Hierarchy 4.0. In each case we are going to review the actual method to approach the issue and the new method, using Hierarchy 4.0, to solve the problem at hand.\\nThe four case studies are 1)Restore a PSD 1 node without requiring a planned shutdown; 2)Evaluate the maintenance on a voting 2 out of 3 instruments; 3)Prepare the maintenance on 2 out of 3 voting where one instrument is already in fault; 4) Prepare a full Root Cause Analysis (RCA) of an unexpected shutdown.', metadata={'source': './KS-all-info_rev1.txt'}),\n",
       " Document(page_content=\"WHAT IS HIERARCHY 4.0\\nwhether you own build manage maintain or operate an oil plant inevitably issues arise that require immediate action and resolution.\\nWith big data flowing in constantly from all sectors making sense of everything while troubleshooting\\nissues without wasting time can be a huge challenge. \\nSo what's the solution?\\nintroducing hierarchy 4.0 and Innovative software solution for control Safety Systems \\nHierarchy 4.0 presents an interactive diagram of the entire plant revealing cause and effect Behavior with readings provided in a hierarchical view allowing for a deep understanding of the system's strategy \\nAll data is collected from multiple sources visualized as a diagram and optimized through a customized dashboard allowing users to run a logic simulation from live data or pick a moment from their history. \\nYour simulation is based on actual safety Logics not just on a math model \\nNow every users can prepare an RCA report 90 percent faster in just a few minutes.\\nHierarchy can be used for any project phase starting from engineering to commissioning and up to operation and maintenance while supporting hazop Hazard analysis by reducing human error and avoiding incorrect documentation.\\nHierarchy 4.0 supports plant operators in decision making taking into account both the safety and the operability of their assets. \\nHierarchy 4.0 Embraces a block log approach: it automatically calculates all Logics affected by an\\noverride and gives a full understanding of constraints. \\nNot convinced let's look at the data! \\nDuring its first project hierarchy 4.0 prevented a revenue loss of 45 million dollars. \\nPlants that utilize hierarchy 4.0 save up to 95 of their time and deliver a return on investment up to five times in value from day one and experience a Personnel utilization and plant efficiency increase by 20 percent per year.\", metadata={'source': './KS-all-info_rev1.txt'}),\n",
       " Document(page_content='Try our demo and make the move to hierarchy 4.0 today', metadata={'source': './KS-all-info_rev1.txt'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Hierarchy 4.0?\"\n",
    "query = \"What is the CASE STUDIES\"\n",
    "docs = db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "# from langchain import HuggingFaceHub\n",
    "# llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
    "# chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What the actual issues and drawbacks ?\"\n",
    "# docs = db.similarity_search(query)\n",
    "# chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=None callbacks=None callback_manager=None verbose=False tags=None metadata=None input_key='input_documents' output_key='output_text' llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), llm=HuggingFaceHub(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=InferenceAPI(api_url='https://api-inference.huggingface.co/pipeline/text2text-generation/declare-lab/flan-alpaca-large', task='text2text-generation', options={'wait_for_model': True, 'use_gpu': False}), repo_id='declare-lab/flan-alpaca-large', task=None, model_kwargs={'temperature': 0, 'max_length': 300}, huggingfacehub_api_token=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}) document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True) document_variable_name='context' document_separator='\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Chain.__call__() got an unexpected keyword argument 'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat is the case study challenge\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m docs \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39msimilarity_search(query)\n\u001b[1;32m----> 8\u001b[0m chain(docs, question\u001b[39m=\u001b[39;49mquery)    \n",
      "\u001b[1;31mTypeError\u001b[0m: Chain.__call__() got an unexpected keyword argument 'question'"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm2=HuggingFaceHub(repo_id=\"declare-lab/flan-alpaca-large\", model_kwargs={\"temperature\":0, \"max_length\": 300})\n",
    "chain = load_qa_chain(llm2, chain_type=\"stuff\")\n",
    "query = \"What is the case study challenge\"\n",
    "docs = db.similarity_search(query)\n",
    "chain(input_documents=docs, question=query)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiQueryRetriever(tags=None, metadata=None, retriever=VectorStoreRetriever(tags=['FAISS'], metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x0000024D3E7D5630>, search_type='similarity', search_kwargs={}), llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}', template_format='f-string', validate_template=True), llm=HuggingFaceHub(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=InferenceAPI(api_url='https://api-inference.huggingface.co/pipeline/text2text-generation/declare-lab/flan-alpaca-large', task='text2text-generation', options={'wait_for_model': True, 'use_gpu': False}), repo_id='declare-lab/flan-alpaca-large', task=None, model_kwargs={'temperature': 0, 'max_length': 300}, huggingfacehub_api_token=None), output_key='text', output_parser=LineListOutputParser(pydantic_object=<class 'langchain.retrievers.multi_query.LineList'>), return_final_only=True, llm_kwargs={}), verbose=True, parser_key='lines')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db.as_retriever(),\n",
    "                                                  llm=llm2)\n",
    "retriever_from_llm\n",
    "# unique_docs = retriever_from_llm.get_relevant_documents(query=query)\n",
    "# unique_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2028"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 is the number of tokens in my text\n",
      "421 is the number of tokens in my text\n",
      "421 is the number of tokens in my text\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens_integer = encoding.encode(docs[0].page_content)\n",
    "print(f\"{len(tokens_integer)} is the number of tokens in my text\")\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "tokens_integer = encoding.encode(docs[0].page_content)\n",
    "print(f\"{len(tokens_integer)} is the number of tokens in my text\")\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens_integer = encoding.encode(docs[0].page_content)\n",
    "print(f\"{len(tokens_integer)} is the number of tokens in my text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Document Loader\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# loader = TextLoader('./KS-all-info_rev1.txt')\n",
    "# documents = loader.load()\n",
    "# import textwrap\n",
    "\n",
    "# def wrap_text_preserve_newlines(text, width=110):\n",
    "#     # Split the input text into lines based on newline characters\n",
    "#     lines = text.split('\\n')\n",
    "\n",
    "#     # Wrap each line individually\n",
    "#     wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "#     # Join the wrapped lines back together using newline characters\n",
    "#     wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "#     return wrapped_text\n",
    "# result = wrap_text_preserve_newlines(str(documents[0]))\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 629/629 [00:00<?, ?B/s] \n",
      "c:\\Users\\damia\\Documents\\GitHub\\TalkDocument\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\damia\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:06<00:00, 38.9MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.09MB/s]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
